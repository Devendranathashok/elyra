{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a92db5c1-2a2c-4c3e-b867-e6c1bd4b40dd",
   "metadata": {},
   "source": [
    "1. Import required libraries and components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ac5e3d-0102-490a-b433-87e58ef8a71d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eab22e0-c462-458e-86c3-79c8ad93d09e",
   "metadata": {},
   "source": [
    "2. Establish variables and parameters that are used throughout the exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31764a4-c7b4-4f62-81d2-a69d10897a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 5              # columns\n",
    "num_informative_features = 3  # columns w/relevant pattern\n",
    "test_size = 0.2               # percent data to reserve for test\n",
    "random_state = 42\n",
    "\n",
    "file_path = 'small_data.csv'\n",
    "num_samples = 2_000           # rows\n",
    "chunk_size = 100              # rows to process at once\n",
    "num_chunks = num_samples // chunk_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05c8bfc-43f4-4154-b758-d04da1ef0039",
   "metadata": {},
   "source": [
    "3. Generate a CSV file that contains randomized data that can fit into system memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191f59cd-9556-4175-9fd5-0442ef5cda84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create CSV file and write header\n",
    "header = [f'feature_{i+1}' for i in range(num_features)] + ['target']\n",
    "with open(file_path, 'w') as f:\n",
    "    f.write(','.join(header) + '\\n')\n",
    "\n",
    "for i in range(num_chunks):\n",
    "    # generate  data with distinct clusters for each class\n",
    "    X, y = make_classification(n_samples=chunk_size, n_features=num_features,\n",
    "                               n_informative=num_informative_features, n_redundant=0,\n",
    "                               n_classes=2, weights=[0.7, 0.3], random_state=random_state)\n",
    "\n",
    "    # create and append data frame to file\n",
    "    df = pd.DataFrame(X, columns=[f'feature_{i+1}' for i in range(num_features)])\n",
    "    df['target'] = y\n",
    "    df.to_csv(file_path, mode='a', header=False, index=False)\n",
    "\n",
    "    # print percentage complete\n",
    "    print(f'\\r\\033[K{100 * (i+1) // num_chunks}%', end='')\n",
    "\n",
    "print(f\"\\nCSV file '{file_path}' generated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a774461f-2d11-4d96-8470-46a847598523",
   "metadata": {},
   "source": [
    "4. Train a model by using the typical, non-streaming technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d46d49-d6f3-4763-bef9-81dd1c4c26d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(file_path)\n",
    "X = data.drop('target', axis=1)\n",
    "y = data['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    random_state=0\n",
    ")\n",
    "model = GaussianNB()\n",
    "model.fit(X_train, y_train)\n",
    "y_predicted = model.predict(X_test)\n",
    "\n",
    "print(\"Classification report:\")\n",
    "print(classification_report(y_test, y_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd939fe-f106-46b9-a554-055de88d5fcc",
   "metadata": {},
   "source": [
    "5. Update the variables and generate a larger CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be2f9e8-db77-4944-ad3e-d3a5c9e7331c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_path = 'big_data.csv'\n",
    "num_samples = 10_000_000       # rows\n",
    "chunk_size = 100_000           # rows to process at once\n",
    "num_chunks = num_samples // chunk_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a691f8-10f4-4751-baf8-7027790886c7",
   "metadata": {},
   "source": [
    "6. Generate a large CSV file.\n",
    "Note that this process can take a few minutes to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9819666-dd2b-4ec7-b5ef-8f844bf4495e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create CSV file and write header\n",
    "header = [f'feature_{i+1}' for i in range(num_features)] + ['target']\n",
    "with open(file_path, 'w') as f:\n",
    "    f.write(','.join(header) + '\\n')\n",
    "\n",
    "for i in range(num_chunks):\n",
    "    # generate  data with distinct clusters for each class\n",
    "    X, y = make_classification(n_samples=chunk_size, n_features=num_features,\n",
    "                               n_informative=num_informative_features, n_redundant=0,\n",
    "                               n_classes=2, weights=[0.7, 0.3], random_state=random_state)\n",
    "\n",
    "    # create and append data frame to file\n",
    "    df = pd.DataFrame(X, columns=[f'feature_{i+1}' for i in range(num_features)])\n",
    "    df['target'] = y\n",
    "    df.to_csv(file_path, mode='a', header=False, index=False)\n",
    "\n",
    "    # print percentage complete\n",
    "    print(f'\\r\\033[K{100 * (i+1) // num_chunks}%', end='')\n",
    "\n",
    "print(f\"\\nCSV file '{file_path}' generated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc604ea7-9f54-42f8-ad36-bb34eb0cfc2d",
   "metadata": {},
   "source": [
    "7. Attempt to train the larger model by using the typical, non-streaming technique.\n",
    "Doing so fails because the kernel crashes when trying to load such a large file into memory all at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e85752-2729-4c7d-b863-9b26ec9d5d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(file_path)\n",
    "X = data.drop('target', axis=1)\n",
    "y = data['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    random_state=0\n",
    ")\n",
    "model = GaussianNB()\n",
    "model.fit(X_train, y_train)\n",
    "y_predicted = model.predict(X_test)\n",
    "\n",
    "print(\"Classification report:\")\n",
    "print(classification_report(y_test, y_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f96630b-3e4a-49c8-95f8-970021954a93",
   "metadata": {},
   "source": [
    "8. Because the Python kernel crashed in the previous step, re-run the `import` instructions and recreate the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b58c293-374b-497c-a868-661aec0dd79d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "num_features = 5              # columns\n",
    "num_informative_features = 3  # columns w/relevant pattern\n",
    "test_size = 0.2               # percent data to reserve for test\n",
    "random_state = 42\n",
    "\n",
    "file_path = 'big_data.csv'\n",
    "num_samples = 10_000_000      # rows\n",
    "chunk_size = 100_000          # rows to process at once\n",
    "num_chunks = num_samples // chunk_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb7dfa6-7c80-40fe-a45e-8aaf994ab8f2",
   "metadata": {},
   "source": [
    "9. Train the model by streaming the data from the file in chunks.\n",
    "By doing so, only some of the data needs to fit into memory at the same time.\n",
    "Note that this process can take a few minutes to complete.\n",
    "\n",
    "> NOTE You must use classifiers that work well with piecemeal, or \"out-of-core\", training.\n",
    "The `GaussianNB` classifier, or Gaussian naive-Bayes, is one such classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942203c8-8de6-4aa1-ac27-512e17dbf10b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = GaussianNB()\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# initial state\n",
    "first_chunk = True\n",
    "count = 1\n",
    "X_train_partial = pd.DataFrame()\n",
    "y_train_partial = pd.Series(dtype=\"float64\")\n",
    "\n",
    "# stream data and train incrementally\n",
    "for chunk in pd.read_csv(file_path, chunksize=chunk_size):\n",
    "    X = chunk.drop('target', axis=1)\n",
    "    y = chunk['target']\n",
    "\n",
    "    X_scaled = scaler.fit_transform(X) if first_chunk else scaler.transform(X)\n",
    "\n",
    "    # split data into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=test_size, random_state=42)\n",
    "\n",
    "    # incrementally train the model\n",
    "    X_train_partial = pd.concat([X_train_partial, pd.DataFrame(X_train)], ignore_index=True)\n",
    "    y_train_partial = pd.concat([y_train_partial, y_train], ignore_index=True)\n",
    "\n",
    "    model.partial_fit(X_train_partial, y_train_partial, classes=y.unique())\n",
    "\n",
    "    # print percentage complete\n",
    "    print(f'\\r\\033[K{100 * count // num_chunks}%', end='')\n",
    "\n",
    "    # update state\n",
    "    first_chunk = False\n",
    "    count += 1\n",
    "\n",
    "y_predicted = model.predict(X_test)\n",
    "print()\n",
    "print(\"Classification report:\")\n",
    "print(classification_report(y_test, y_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b4c047-571f-4545-aa32-89f6b65276e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
