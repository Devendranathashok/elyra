{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a92db5c1-2a2c-4c3e-b867-e6c1bd4b40dd",
   "metadata": {},
   "source": [
    "1. Import required libraries and components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8ac5e3d-0102-490a-b433-87e58ef8a71d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from data.generation import generate_synthetic_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eab22e0-c462-458e-86c3-79c8ad93d09e",
   "metadata": {},
   "source": [
    "2. Establish variables and parameters that are used throughout the exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e31764a4-c7b4-4f62-81d2-a69d10897a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'data.csv'\n",
    "num_features = 500            # columns\n",
    "num_informative_features = 250  # columns w/relevant pattern\n",
    "num_samples = 1_000          # rows\n",
    "\n",
    "test_size = 0.2               # percent data to reserve for test\n",
    "random_state = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05c8bfc-43f4-4154-b758-d04da1ef0039",
   "metadata": {},
   "source": [
    "3. Generate a CSV file that contains randomized data that can fit into system memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "191f59cd-9556-4175-9fd5-0442ef5cda84",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K100%\n",
      "CSV file 'data.csv' generated\n"
     ]
    }
   ],
   "source": [
    "generate_synthetic_dataset(num_features, num_samples, file_path, num_informative_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a774461f-2d11-4d96-8470-46a847598523",
   "metadata": {},
   "source": [
    "4. Train a model by using the typical, non-streaming technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2d46d49-d6f3-4763-bef9-81dd1c4c26d5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.88      0.86       148\n",
      "           1       0.61      0.54      0.57        52\n",
      "\n",
      "    accuracy                           0.79       200\n",
      "   macro avg       0.73      0.71      0.72       200\n",
      "weighted avg       0.78      0.79      0.79       200\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jairamir/dev/AI26X-apps/models/practices-data/.venv/lib64/python3.9/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "data = pd.read_csv(file_path)\n",
    "X = data.drop('target', axis=1)\n",
    "y = data['target']\n",
    "\n",
    "# Generate train and test splits\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "# Train a small neural network, which includes 2 layers of 10 neurons\n",
    "model = MLPClassifier(hidden_layer_sizes=(3, 2), random_state=1)\n",
    "model.fit(X_train, y_train)\n",
    "y_predicted = model.predict(X_test)\n",
    "\n",
    "print(\"Classification report:\")\n",
    "print(classification_report(y_test, y_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11f3c0aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.92      0.92       148\n",
      "           1       0.76      0.75      0.76        52\n",
      "\n",
      "    accuracy                           0.88       200\n",
      "   macro avg       0.84      0.83      0.84       200\n",
      "weighted avg       0.87      0.88      0.87       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train a larger neural network, which includes 4 layers of 1000 neurons\n",
    "model = MLPClassifier(hidden_layer_sizes=(1000, 1000, 1000, 1000), random_state=1)\n",
    "model.fit(X_train, y_train)\n",
    "y_predicted = model.predict(X_test)\n",
    "\n",
    "print(\"Classification report:\")\n",
    "print(classification_report(y_test, y_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23b6c6d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K100%\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.88      0.90       148\n",
      "           1       0.70      0.81      0.75        52\n",
      "\n",
      "    accuracy                           0.86       200\n",
      "   macro avg       0.81      0.84      0.83       200\n",
      "weighted avg       0.87      0.86      0.86       200\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def read_data_in_batches(X: np.array, y: np.array, batch_size: int):\n",
    "    num_batches = get_num_batches(X, batch_size)\n",
    "\n",
    "    for i in range(num_batches):\n",
    "        batch_start = i * batch_size\n",
    "        batch_end = batch_start + batch_size\n",
    "        yield X[batch_start:batch_end], y[batch_start:batch_end]\n",
    "\n",
    "def get_num_batches(array: np.array, batch_size: int):\n",
    "    num_samples = array.shape[0]\n",
    "    # rounded division (//) returns int\n",
    "    return num_samples // batch_size\n",
    "\n",
    "count = 1\n",
    "batch_size = 10\n",
    "num_batches = get_num_batches(X_train, batch_size)\n",
    "model = MLPClassifier(hidden_layer_sizes=(1000, 1000, 1000, 1000), random_state=1)\n",
    "\n",
    "# stream data and train incrementally\n",
    "for X_batch, y_batch in read_data_in_batches(X_train, y_train, batch_size):\n",
    "\n",
    "    model.partial_fit(X_batch, y_batch, classes=y.unique())\n",
    "\n",
    "    # print percentage complete\n",
    "    print(f'\\r\\033[K{100 * count // num_batches}%', end='')\n",
    "\n",
    "    count += 1\n",
    "\n",
    "y_predicted = model.predict(X_test)\n",
    "print()\n",
    "print(\"Classification report:\")\n",
    "print(classification_report(y_test, y_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd939fe-f106-46b9-a554-055de88d5fcc",
   "metadata": {},
   "source": [
    "5. Update the variables and generate a larger CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be2f9e8-db77-4944-ad3e-d3a5c9e7331c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_path = 'big_data.csv'\n",
    "num_samples = 400_000       # rows\n",
    "chunk_size = 100_000           # rows to process at once\n",
    "num_chunks = num_samples // chunk_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a691f8-10f4-4751-baf8-7027790886c7",
   "metadata": {},
   "source": [
    "6. Generate a large CSV file.\n",
    "Note that this process can take a few minutes to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9819666-dd2b-4ec7-b5ef-8f844bf4495e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create CSV file and write header\n",
    "header = [f'feature_{i+1}' for i in range(num_features)] + ['target']\n",
    "with open(file_path, 'w') as f:\n",
    "    f.write(','.join(header) + '\\n')\n",
    "\n",
    "for i in range(num_chunks):\n",
    "    # generate  data with distinct clusters for each class\n",
    "    X, y = make_classification(n_samples=chunk_size, n_features=num_features,\n",
    "                               n_informative=num_informative_features, n_redundant=0,\n",
    "                               n_classes=2, weights=[0.7, 0.3], random_state=random_state)\n",
    "\n",
    "    # create and append data frame to file\n",
    "    df = pd.DataFrame(X, columns=[f'feature_{i+1}' for i in range(num_features)])\n",
    "    df['target'] = y\n",
    "    df.to_csv(file_path, mode='a', header=False, index=False)\n",
    "\n",
    "    # print percentage complete\n",
    "    print(f'\\r\\033[K{100 * (i+1) // num_chunks}%', end='')\n",
    "\n",
    "print(f\"\\nCSV file '{file_path}' generated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc604ea7-9f54-42f8-ad36-bb34eb0cfc2d",
   "metadata": {},
   "source": [
    "7. Attempt to train the larger model by using the typical, non-streaming technique.\n",
    "Doing so fails because the kernel crashes when trying to load such a large file into memory all at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e85752-2729-4c7d-b863-9b26ec9d5d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(file_path)\n",
    "X = data.drop('target', axis=1)\n",
    "y = data['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    random_state=0\n",
    ")\n",
    "model = MLPClassifier(hidden_layer_sizes=(1000,1000,1000), random_state=1)\n",
    "model.fit(X_train, y_train)\n",
    "y_predicted = model.predict(X_test)\n",
    "\n",
    "print(\"Classification report:\")\n",
    "print(classification_report(y_test, y_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f96630b-3e4a-49c8-95f8-970021954a93",
   "metadata": {},
   "source": [
    "8. Because the Python kernel crashed in the previous step, re-run the `import` instructions and recreate the variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b58c293-374b-497c-a868-661aec0dd79d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "num_features = 5              # columns\n",
    "num_informative_features = 3  # columns w/relevant pattern\n",
    "test_size = 0.2               # percent data to reserve for test\n",
    "random_state = 42\n",
    "\n",
    "file_path = 'big_data.csv'\n",
    "num_samples = 10_000_000      # rows\n",
    "chunk_size = 100_000          # rows to process at once\n",
    "num_chunks = num_samples // chunk_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb7dfa6-7c80-40fe-a45e-8aaf994ab8f2",
   "metadata": {},
   "source": [
    "9. Train the model by streaming the data from the file in chunks.\n",
    "By doing so, only some of the data needs to fit into memory at the same time.\n",
    "Note that this process can take a few minutes to complete.\n",
    "\n",
    "> NOTE You must use classifiers that work well with piecemeal, or \"out-of-core\", training.\n",
    "The `GaussianNB` classifier, or Gaussian naive-Bayes, is one such classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942203c8-8de6-4aa1-ac27-512e17dbf10b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = GaussianNB()\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# initial state\n",
    "first_chunk = True\n",
    "count = 1\n",
    "X_train_partial = pd.DataFrame()\n",
    "y_train_partial = pd.Series(dtype=\"float64\")\n",
    "\n",
    "# stream data and train incrementally\n",
    "for chunk in pd.read_csv(file_path, chunksize=chunk_size):\n",
    "    X = chunk.drop('target', axis=1)\n",
    "    y = chunk['target']\n",
    "\n",
    "    X_scaled = scaler.fit_transform(X) if first_chunk else scaler.transform(X)\n",
    "\n",
    "    # split data into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=test_size, random_state=42)\n",
    "\n",
    "    # incrementally train the model\n",
    "    X_train_partial = pd.concat([X_train_partial, pd.DataFrame(X_train)], ignore_index=True)\n",
    "    y_train_partial = pd.concat([y_train_partial, y_train], ignore_index=True)\n",
    "\n",
    "    model.partial_fit(X_train_partial, y_train_partial, classes=y.unique())\n",
    "\n",
    "    # print percentage complete\n",
    "    print(f'\\r\\033[K{100 * count // num_chunks}%', end='')\n",
    "\n",
    "    # update state\n",
    "    first_chunk = False\n",
    "    count += 1\n",
    "\n",
    "y_predicted = model.predict(X_test)\n",
    "print()\n",
    "print(\"Classification report:\")\n",
    "print(classification_report(y_test, y_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b4c047-571f-4545-aa32-89f6b65276e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
